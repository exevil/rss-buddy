"""Unit tests for the generate_pages module."""

import hashlib
import html
import json
import os
import sys
import tempfile
import unittest
from datetime import datetime, timedelta, timezone
from unittest.mock import patch

# Add src directory to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "src")))

# Modules to test
from rss_buddy import generate_pages


class TestGeneratePages(unittest.TestCase):
    """Tests for the generate_pages functionality."""

    def setUp(self):
        """Set up temporary directories for data and output."""
        self.temp_root = tempfile.TemporaryDirectory()
        self.data_dir = os.path.join(self.temp_root.name, "data")
        self.output_dir = os.path.join(self.temp_root.name, "output")
        os.makedirs(self.data_dir)
        # output_dir should be created by the function being tested

    def tearDown(self):
        """Clean up temporary directories."""
        self.temp_root.cleanup()

    def _write_state(self, state_data: dict):
        """Helper to write a state dictionary to the temp data_dir."""
        state_file = os.path.join(self.data_dir, "processed_state.json")
        with open(state_file, "w") as f:
            json.dump(state_data, f)
        return state_file

    def _create_dummy_state(self, num_feeds=1, items_per_feed=2, days_lookback=7) -> dict:
        """Creates a usable dummy state dictionary."""
        state = {"feeds": {}, "last_updated": datetime.now(timezone.utc).isoformat()}
        now = datetime.now(timezone.utc)

        for i in range(num_feeds):
            feed_url = f"https://example.com/feed{i}.xml"
            feed_title = f"Test Feed Title {i}"
            feed_data = {"entry_data": {}, "feed_title": feed_title, "last_entry_date": None}
            entry_ids = []
            for j in range(items_per_feed):
                entry_id = f"item_{i}_{j}"
                entry_ids.append(entry_id)
                # Alternate between recent and old, processed and digest
                is_recent = j % 2 == 0
                status = "digest" if (i + j) % 3 == 0 else "processed"
                item_date = (
                    now - timedelta(days=days_lookback // 2)
                    if is_recent
                    else now - timedelta(days=days_lookback * 2)
                )

                feed_data["entry_data"][entry_id] = {
                    "id": entry_id,
                    "title": f"Feed {i} Item {j} ({status})",
                    "link": f"https://example.com/feed{i}/item{j}",
                    "summary": f"Summary for item {j} of feed {i}.",
                    "date": item_date.isoformat(),
                    "processed_at": now.isoformat(),
                    "status": status,
                }
                if is_recent and feed_data["last_entry_date"] is None:
                    feed_data["last_entry_date"] = item_date.isoformat()

            state["feeds"][feed_url] = feed_data

        return state

    # --- Test HTML Helper Functions ---

    def test_create_html_header(self):
        """Verify basic structure of HTML header."""
        title = "Test Title <>&"
        html = generate_pages.create_html_header(title)
        self.assertIn("<title>Test Title &lt;&gt;&amp;</title>", html)
        self.assertIn("<style>", html)
        self.assertIn("</head>", html)

    def test_create_index_html_start(self):
        """Verify basic structure of index start."""
        html = generate_pages.create_index_html_start()
        self.assertIn("<h1>RSS Buddy Processed Feeds</h1>", html)
        self.assertIn('<ul class="index-list">', html)

    def test_create_feed_html_start(self):
        """Verify basic structure of feed page start."""
        title = "Feed Title <>&"
        html = generate_pages.create_feed_html_start(title)
        self.assertIn("<h1>Feed Title &lt;&gt;&amp;</h1>", html)
        self.assertIn('<a href="index.html" class="back-link">', html)

    def test_create_html_footer(self):
        """Verify basic structure of HTML footer."""
        html = generate_pages.create_html_footer()
        self.assertIn("<footer>", html)
        self.assertIn("Generated by RSS Buddy", html)
        self.assertIn("</footer>", html)

    # --- Test format_item_html ---

    def test_format_item_html_processed(self):
        """Test formatting a standard processed item."""
        now = datetime.now(timezone.utc)
        item = {
            "id": "item1",
            "title": "Test Title < >",
            "link": "http://l.co",
            "summary": "<p>Summary</p>",
            "date": now.isoformat(),
            "processed_at": (now - timedelta(minutes=5)).isoformat(),
            "status": "processed",
        }
        html = generate_pages.format_item_html(item, is_digest_item=False)
        self.assertIn('<div class="article">', html)
        self.assertIn('<h2><a href="http://l.co">Test Title &lt; &gt;</a></h2>', html)
        self.assertIn('<div class="article-meta">Published:', html)
        self.assertIn("Processed:", html)  # Check processed date is included
        self.assertIn("<p>Summary</p>", html)  # Raw HTML in summary should be preserved

    def test_format_item_html_digest_item(self):
        """Test formatting an item belonging to a digest."""
        now = datetime.now(timezone.utc)
        item = {
            "id": "item2",
            "title": "Digest Item",
            "link": "http://d.co",
            "summary": "Digest summary",
            "date": now.isoformat(),
            "status": "digest",  # Status from item itself isn't used here, only is_digest_item flag
        }
        # Note: This tests formatting an *individual* item as if it *were* a digest section
        # This is slightly different from the digest summary wrapper itself.
        html = generate_pages.format_item_html(item, is_digest_item=True)
        self.assertIn('<div class="digest">', html)  # Uses digest class
        self.assertIn('<h3><a href="http://d.co">Digest Item</a></h3>', html)  # Uses H3

    def test_format_item_html_missing_data(self):
        """Test formatting with missing link, summary, date."""
        item = {"id": "item3", "title": "Minimal Item"}
        html = generate_pages.format_item_html(item)
        self.assertIn('<h2><a href="#">Minimal Item</a></h2>', html)  # Defaults link to #
        self.assertIn("Published: Date not available", html)
        self.assertNotIn("Processed:", html)  # No processed date
        # Check that the div for summary content exists but is empty or just whitespace
        self.assertRegex(html, r"<div>\s*</div>")

    def test_format_item_html_unparseable_date(self):
        """Test formatting with an unparseable date string."""
        item = {"id": "item4", "title": "Bad Date", "date": "yesterday maybe?"}
        html = generate_pages.format_item_html(item)
        self.assertIn("Published: Original: yesterday maybe?", html)

    # --- Test _copy_state_file ---

    def test_copy_state_file(self):
        """Test that the state file is copied correctly."""
        state_data = self._create_dummy_state()
        source_path = self._write_state(state_data)
        dest_dir = self.output_dir
        os.makedirs(dest_dir)  # Needs to exist for copy

        generate_pages._copy_state_file(self.data_dir, dest_dir)

        dest_path = os.path.join(dest_dir, "processed_state.json")
        self.assertTrue(os.path.exists(dest_path))
        # Optional: Compare content, but existence is usually enough
        with open(source_path, "r") as f_src, open(dest_path, "r") as f_dest:
            self.assertEqual(json.load(f_src), json.load(f_dest))

    def test_copy_state_file_source_missing(self):
        """Test copy behavior when source state file doesn't exist."""
        dest_dir = self.output_dir
        os.makedirs(dest_dir)
        # No state file written to self.data_dir
        generate_pages._copy_state_file(self.data_dir, dest_dir)
        dest_path = os.path.join(dest_dir, "processed_state.json")
        self.assertFalse(os.path.exists(dest_path))  # Should not create empty file

    # --- Test _generate_feed_html (Integration style via generate_pages) ---
    # We test the main generate_pages function which calls _generate_feed_html

    @patch("rss_buddy.generate_pages._generate_feed_html")
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_basic(self, mock_get_int, mock_get_str, mock_generate_feed_html):
        """Test generating pages orchestration for a simple state file."""
        # Setup State FIRST to get feed_url
        state_data = self._create_dummy_state(num_feeds=1, items_per_feed=2, days_lookback=7)
        feed_url = list(state_data["feeds"].keys())[0]
        # Manually set statuses for predictability
        item_ids = list(state_data["feeds"][feed_url]["entry_data"].keys())
        state_data["feeds"][feed_url]["entry_data"][item_ids[0]]["status"] = (
            "digest"  # Item 0 is digest
        )
        state_data["feeds"][feed_url]["entry_data"][item_ids[1]]["status"] = (
            "processed"  # Item 1 is processed
        )
        # Ensure one item is old (item 1)
        state_data["feeds"][feed_url]["entry_data"][item_ids[1]]["date"] = (
            datetime.now(timezone.utc) - timedelta(days=10)
        ).isoformat()
        self._write_state(state_data)

        # Mock the helper to return basic metadata (using feed_url defined above)
        feed_hash = hashlib.md5(feed_url.encode()).hexdigest()
        mock_generate_feed_html.return_value = {
            "filename": f"feed_{feed_hash}.html",
            "title": "Test Feed 0",
            "processed_count": 1,
            "digest_count": 1,
            "last_updated": datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M GMT"),
            "url": feed_url,
        }

        # Run Generation
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        # 1. Output directory created
        self.assertTrue(os.path.isdir(self.output_dir))

        # 2. Index file created
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        with open(index_path, "r") as f:
            index_content = f.read()
        self.assertIn(f'<a href="feed_{feed_hash}.html">', index_content)
        self.assertIn("Test Feed 0", index_content)

        # 3. Feed file creation is now handled by the mocked helper, so we check the helper was called
        mock_generate_feed_html.assert_called_once()
        call_args = mock_generate_feed_html.call_args  # Get the call object
        # Check kwargs passed to helper
        self.assertEqual(call_args.kwargs["feed_url"], feed_url)
        self.assertIsInstance(call_args.kwargs["state_manager"], generate_pages.StateManager)
        self.assertIsInstance(call_args.kwargs["ai_interface"], generate_pages.AIInterface)
        self.assertEqual(call_args.kwargs["days_lookback"], 7)  # From mock_get_int
        self.assertEqual(call_args.kwargs["summary_max_tokens"], 7)  # From mock_get_int
        self.assertEqual(call_args.kwargs["output_dir"], self.output_dir)

        # 4. State file copied
        copied_state_path = os.path.join(self.output_dir, "processed_state.json")
        self.assertTrue(os.path.exists(copied_state_path))

    @patch("rss_buddy.generate_pages._generate_feed_html")
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_with_digest(self, mock_get_int, mock_get_str, mock_generate_feed_html):
        """Test orchestration includes digest scenario."""
        # Setup State FIRST
        state_data = self._create_dummy_state(num_feeds=1, items_per_feed=2, days_lookback=7)
        feed_url = list(state_data["feeds"].keys())[0]
        item_ids = list(state_data["feeds"][feed_url]["entry_data"].keys())
        state_data["feeds"][feed_url]["entry_data"][item_ids[0]]["status"] = "processed"
        state_data["feeds"][feed_url]["entry_data"][item_ids[1]]["status"] = "digest"
        state_data["feeds"][feed_url]["entry_data"][item_ids[0]]["date"] = (
            datetime.now(timezone.utc) - timedelta(days=1)
        ).isoformat()
        state_data["feeds"][feed_url]["entry_data"][item_ids[1]]["date"] = (
            datetime.now(timezone.utc) - timedelta(days=2)
        ).isoformat()
        self._write_state(state_data)

        # Mock the helper (using feed_url defined above)
        feed_hash = hashlib.md5(feed_url.encode()).hexdigest()
        mock_generate_feed_html.return_value = {
            "filename": f"feed_{feed_hash}.html",
            "title": "Test Feed 0 With Digest",
            "processed_count": 1,
            "digest_count": 1,
            "last_updated": datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M GMT"),
            "url": feed_url,
        }

        # Run Generation
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        mock_generate_feed_html.assert_called_once()
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        # feed_path = os.path.join(self.output_dir, f"feed_{feed_hash}.html") # Not created by mock
        # self.assertTrue(os.path.exists(feed_path)) # Don't assert file existence

    @patch("rss_buddy.generate_pages._generate_feed_html")
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_ai_digest_fails(
        self, mock_get_int, mock_get_str, mock_generate_feed_html
    ):
        """Test orchestration handles AI digest failure scenario."""
        # Setup State FIRST
        state_data = self._create_dummy_state(num_feeds=1, items_per_feed=1, days_lookback=7)
        feed_url = list(state_data["feeds"].keys())[0]
        item_ids = list(state_data["feeds"][feed_url]["entry_data"].keys())
        state_data["feeds"][feed_url]["entry_data"][item_ids[0]]["status"] = "digest"
        state_data["feeds"][feed_url]["entry_data"][item_ids[0]]["date"] = (
            datetime.now(timezone.utc) - timedelta(days=1)
        ).isoformat()
        self._write_state(state_data)

        # Mock the helper (using feed_url defined above)
        feed_hash = hashlib.md5(feed_url.encode()).hexdigest()
        mock_generate_feed_html.return_value = {
            "filename": f"feed_{feed_hash}.html",
            "title": "Feed With Failing Digest",
            "processed_count": 0,
            "digest_count": 1,
            "last_updated": datetime.now(timezone.utc).strftime("%a, %d %b %Y %H:%M GMT"),
            "url": feed_url,
        }  # Simulate data returned even on internal failure

        # Run Generation
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        mock_generate_feed_html.assert_called_once()
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        # feed_path = os.path.join(self.output_dir, f"feed_{feed_hash}.html") # Not created by mock
        # self.assertTrue(os.path.exists(feed_path)) # Don't assert file existence

    @patch("rss_buddy.generate_pages._generate_feed_html")
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_no_recent_items(
        self, mock_get_int, mock_get_str, mock_generate_feed_html
    ):
        """Test generation when no items are within the lookback period."""
        # Mock the helper to return None, as it does when no recent items are found
        mock_generate_feed_html.return_value = None

        # Setup State (all items old)
        state_data = self._create_dummy_state(num_feeds=1, items_per_feed=2, days_lookback=7)
        feed_url = list(state_data["feeds"].keys())[0]
        feed_hash = hashlib.md5(feed_url.encode()).hexdigest()  # Restore calculation
        for item_id in state_data["feeds"][feed_url]["entry_data"]:
            state_data["feeds"][feed_url]["entry_data"][item_id]["date"] = (
                datetime.now(timezone.utc) - timedelta(days=10)
            ).isoformat()
        self._write_state(state_data)

        # Run Generation
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        # Index file should be created but list the feed as having no recent items
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        with open(index_path, "r") as f:
            index_content = f.read()
        # Check that the feed link does NOT exist because the helper returned None
        self.assertNotIn(f'<a href="feed_{feed_hash}.html">', index_content)

        # Feed file should NOT be created because helper returned None
        feed_path = os.path.join(self.output_dir, f"feed_{feed_hash}.html")
        self.assertFalse(os.path.exists(feed_path))
        # self.assertIn("No individually processed articles in the lookback period.", feed_content) # Can't check content
        # self.assertIn("No articles marked for digest in the lookback period.", feed_content) # Can't check content

    @patch("rss_buddy.generate_pages._generate_feed_html")
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_empty_state(self, mock_get_int, mock_get_str, mock_generate_feed_html):
        """Test generation with an empty state file."""
        # Setup Mock AI (doesn't need specific methods for this test, just needs to not fail init)
        mock_generate_feed_html.return_value = None

        state_data = {"feeds": {}, "last_updated": datetime.now(timezone.utc).isoformat()}
        self._write_state(state_data)

        # Run Generation
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        # Index file created, but empty list
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        with open(index_path, "r") as f:
            index_content = f.read()
        self.assertNotIn('<a href="feed_', index_content)

        # No feed files created
        # Check output directory contains index.html, processed_state.json, feeds.json, metadata.json
        files_in_output = os.listdir(self.output_dir)
        self.assertIn("index.html", files_in_output)
        self.assertIn("processed_state.json", files_in_output)
        self.assertIn("feeds.json", files_in_output)
        self.assertIn("metadata.json", files_in_output)
        self.assertEqual(len(files_in_output), 4)

    # --- Test Actual Filename Generation ---

    @patch(
        "rss_buddy.ai_interface.AIInterface.generate_consolidated_summary",
        return_value="Mock Digest",
    )
    @patch("rss_buddy.generate_pages.get_env_str", return_value="dummy_key_or_model")
    @patch("rss_buddy.generate_pages.get_env_int", return_value=7)
    def test_generate_pages_uses_feed_title_for_filename(
        self, mock_get_int, mock_get_str, mock_ai_summary
    ):
        """Test that the generated HTML filename uses the sanitized feed title from the state."""
        # Setup state with a specific title needing sanitization
        feed_url = "https://unique-feed.com/rss"
        feed_title_raw = "My Awesome Feed! (Special Chars /\\?)"
        state_data = {
            "feeds": {
                feed_url: {
                    "feed_title": feed_title_raw,
                    "entry_data": {
                        "item1": {
                            "id": "item1",
                            "title": "Recent Processed Item",
                            "link": "http://l.co/1",
                            "summary": "Summary 1",
                            "date": (datetime.now(timezone.utc) - timedelta(days=1)).isoformat(),
                            "processed_at": datetime.now(timezone.utc).isoformat(),
                            "status": "processed",
                        },
                        "item2": {
                            "id": "item2",
                            "title": "Recent Digest Item",
                            "link": "http://l.co/2",
                            "summary": "Summary 2",
                            "date": (datetime.now(timezone.utc) - timedelta(days=2)).isoformat(),
                            "processed_at": datetime.now(timezone.utc).isoformat(),
                            "status": "digest",
                        },
                    },
                }
            },
            "last_updated": datetime.now(timezone.utc).isoformat(),
        }
        self._write_state(state_data)

        # Calculate expected sanitized filename
        expected_sanitized_title = generate_pages.sanitize_filename(feed_title_raw)
        expected_filename = f"feed_{expected_sanitized_title}.html"

        # Run Generation (without mocking _generate_feed_html)
        generate_pages.generate_pages(data_dir=self.data_dir, output_dir=self.output_dir)

        # Assertions
        # 1. Check if the correctly named HTML file exists
        expected_filepath = os.path.join(self.output_dir, expected_filename)
        self.assertTrue(
            os.path.exists(expected_filepath), f"Expected file {expected_filename} not found."
        )

        # 2. Check index.html links to the correct file
        index_path = os.path.join(self.output_dir, "index.html")
        self.assertTrue(os.path.exists(index_path))
        with open(index_path, "r") as f:
            index_content = f.read()
        self.assertIn(f'<a href="{expected_filename}">', index_content)
        self.assertIn(
            html.escape(feed_title_raw), index_content
        )  # Check title is displayed correctly too

        # 3. Check feeds.json contains the correct filename
        feeds_json_path = os.path.join(self.output_dir, "feeds.json")
        self.assertTrue(os.path.exists(feeds_json_path))
        with open(feeds_json_path, "r") as f:
            feeds_metadata = json.load(f)
        self.assertEqual(len(feeds_metadata), 1)
        self.assertEqual(feeds_metadata[0]["url"], feed_url)
        self.assertEqual(feeds_metadata[0]["title"], feed_title_raw)
        self.assertEqual(feeds_metadata[0]["filename"], expected_filename)


if __name__ == "__main__":
    unittest.main()
